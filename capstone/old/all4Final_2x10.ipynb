{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Dense\n",
    "from keras import regularizers\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "import numpy as np\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Folder for the dataset\n",
    "datasetFolder = '/home/carnd/dbpedia2016/all4Final_2x10/dataset/'\n",
    "\n",
    "#Number of files\n",
    "numberOfFiles = 638\n",
    "\n",
    "#Test split\n",
    "testSplit=0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def load_data(datasetFolder, datasetXFile, datasetYFile, wrap=True):\n",
    "    #print('Loading {} & {}'.format(datasetXFile, datasetYFile))\n",
    "    # load file\n",
    "    with open(datasetFolder + datasetXFile, \"r\") as f:\n",
    "        head = f.readline()\n",
    "        cols = head.split(',')\n",
    "        numberOfCols = len(cols)\n",
    "        #print(numberOfCols)\n",
    "        numberOfRows=0\n",
    "        for line in f:\n",
    "            numberOfRows+=1\n",
    "        f.close()\n",
    "    \n",
    "    #print('{} x {}'.format(numberOfRows,numberOfCols))\n",
    "    if(wrap==True):\n",
    "        maxY = 1000\n",
    "    else:\n",
    "        maxY = numberOfCols-1\n",
    "    dataX = np.zeros([numberOfRows,maxY],np.int8)\n",
    "    with open(datasetFolder + datasetXFile, \"r\") as f:\n",
    "        head = f.readline()\n",
    "        rowCounter=0\n",
    "        for line in f:\n",
    "            row=line.split(',')\n",
    "            for i in range(1, len(row)):\n",
    "                if(int(row[i])<=0):\n",
    "                    continue;\n",
    "                dataX[rowCounter][(int(row[i])-1)%maxY]=1 + ((int(row[i])-1)//maxY)\n",
    "                #if((1 + ((int(row[i])-1)//maxY))>1):\n",
    "                #    print(\"{} data[{}][{}] = {}\".format(int(row[i])-1, rowCounter,(int(row[i])-1)%maxY,1 + ((int(row[i])-1)//maxY)))\n",
    "            rowCounter+=1\n",
    "        f.close()\n",
    "        \n",
    "    #print('Loading {} & {}'.format(datasetXFile, datasetYFile))\n",
    "    # load file\n",
    "    with open(datasetFolder + datasetYFile, \"r\") as f:\n",
    "        head = f.readline()\n",
    "        cols = head.split(',')\n",
    "        numberOfCols = len(cols)\n",
    "        #print(numberOfCols)\n",
    "        numberOfRows=0\n",
    "        for line in f:\n",
    "            numberOfRows+=1\n",
    "        f.close()\n",
    "\n",
    "    dataY = np.zeros([numberOfRows,(numberOfCols-1)],np.int8)\n",
    "    with open(datasetFolder + datasetYFile, \"r\") as f:\n",
    "        head = f.readline()\n",
    "        rowCounter=0\n",
    "        for line in f:\n",
    "            row=line.split(',')\n",
    "            for i in range(1, len(row)):\n",
    "                if(int(row[i])<=0):\n",
    "                    continue;\n",
    "                dataY[rowCounter][(int(row[i])-1)]=1\n",
    "            rowCounter+=1\n",
    "        f.close()\n",
    "\n",
    "    return dataX, dataY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1360 is out of bounds for axis 0 with size 525",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-4293553ec96f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasetFolder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'datasetX_1.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'datasetY_1.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-42-f2a14a0f382b>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(datasetFolder, datasetXFile, datasetYFile, wrap)\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m<=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                 \u001b[0mdataY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrowCounter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m             \u001b[0mrowCounter\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1360 is out of bounds for axis 0 with size 525"
     ]
    }
   ],
   "source": [
    "dataX, dataY = load_data(datasetFolder,'datasetX_1.csv', 'datasetY_1.csv', wrap=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Features: 1059283\n",
      "Output Classes: 1059283\n"
     ]
    }
   ],
   "source": [
    "print(\"Input Features: {}\".format(dataX.shape[1]))\n",
    "print(\"Output Classes: {}\".format(dataY.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/ipykernel/__main__.py:19: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    }
   ],
   "source": [
    "dataX, dataY = load_data(datasetFolder,'datasetX_1.csv', 'datasetY_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1000)\n",
      "[[1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0]]\n"
     ]
    }
   ],
   "source": [
    "print(dataX.shape)\n",
    "print(dataX[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(dataY.shape)\n",
    "print(dataY[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(\"Input Features for classification: {}\".format(dataX.shape[1]))\n",
    "print(\"Output Classes for classification: {}\".format(dataY.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create model\n",
    "#logisticRegression = Sequential(name='Simple Logistic Regression')\n",
    "#logisticRegression.add(Dense(dataY.shape[1], input_dim=dataX.shape[1], activation='sigmoid', init='glorot_uniform'))\n",
    "\n",
    "# simpleModel = Sequential(name='2 Fully Connected Layers')\n",
    "# simpleModel.add(Dense(1024, input_dim=dataX.shape[1], activation='relu', init='glorot_uniform'))\n",
    "# simpleModel.add(Dense(dataY.shape[1], activation='sigmoid', init='glorot_uniform'))\n",
    "\n",
    "deepModel = Sequential(name='Deep Model (5 Dense Layers)')\n",
    "deepModel.add(Dense(2048, input_dim=dataX.shape[1], activation='elu', init='glorot_normal'))\n",
    "deepModel.add(BatchNormalization())\n",
    "deepModel.add(LeakyReLU())\n",
    "deepModel.add(Dense(2048, init='glorot_normal'))\n",
    "deepModel.add(BatchNormalization())\n",
    "deepModel.add(LeakyReLU())\n",
    "deepModel.add(Dense(2048, init='glorot_normal'))\n",
    "deepModel.add(BatchNormalization())\n",
    "deepModel.add(LeakyReLU())\n",
    "deepModel.add(Dense(2048, init='glorot_normal'))\n",
    "deepModel.add(BatchNormalization())\n",
    "deepModel.add(LeakyReLU())\n",
    "deepModel.add(Dense(dataY.shape[1], activation='sigmoid', init='glorot_uniform'))\n",
    "\n",
    "models = [deepModel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Compile model\n",
    "import keras.backend as K\n",
    "\n",
    "def count_predictions(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    return true_positives, predicted_positives, possible_positives\n",
    "\n",
    "def f1score(y_true, y_pred):\n",
    "    true_positives, predicted_positives, possible_positives = count_predictions(y_true, y_pred)\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1score = 2.0 * precision * recall / (precision+recall+ K.epsilon())\n",
    "    return f1score\n",
    "\n",
    "def fBetaScore(y_true, y_pred, beta):\n",
    "    true_positives, predicted_positives, possible_positives = count_predictions(y_true, y_pred)\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1score = (1+(beta*beta)) * precision * recall / ((beta*beta*precision)+recall+ K.epsilon())\n",
    "    return f1score\n",
    "\n",
    "for model in models:\n",
    "    model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=[f1score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def fit_data(model, dataX, dataY):\n",
    "    # Fit the model\n",
    "    #model.fit(dataX, dataY, nb_epoch=5, verbose=2, batch_size=256)\n",
    "    return model.train_on_batch(dataX, dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def countPredictions(y_true, y_pred):\n",
    "    true_positives = np.sum(np.round(y_pred*y_true))\n",
    "    predicted_positives = np.sum(np.round(y_pred))\n",
    "    possible_positives = np.sum(y_true)\n",
    "    return true_positives, predicted_positives, possible_positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Randomize the list of numbers so we can split train and test dataset\n",
    "listOfFiles=list(range(1,numberOfFiles+1))\n",
    "import random\n",
    "random.shuffle(listOfFiles)\n",
    "splitIndex=int((1-testSplit)*numberOfFiles)\n",
    "\n",
    "numberOfEons = 25\n",
    "for eon in range(0, numberOfEons):\n",
    "    print('{}. Eon {}/{}'.format(eon+1,eon+1, numberOfEons))\n",
    "    for trainIndex in range(0,splitIndex):\n",
    "        dataX, dataY = load_data(datasetFolder,'datasetX_{}.csv'.format(listOfFiles[trainIndex]), 'datasetY_{}.csv'.format(listOfFiles[trainIndex]))\n",
    "        for model in models:\n",
    "            #print('Model = {}'.format(model.name))\n",
    "            #model.fit(dataX, dataY, nb_epoch=5, verbose=0, batch_size=512)\n",
    "            #sc=model.test_on_batch(dataX,dataY)\n",
    "            #loss = sc[0]\n",
    "            #f1score = sc[1]\n",
    "            loss, f1score=fit_data(model,dataX, dataY)\n",
    "            print('Learning for file {} / {} : datasetX/Y_{}\\t\\tloss={:.4f} f1score={:.4f}'.format(trainIndex+1, splitIndex, listOfFiles[trainIndex], loss, f1score), end='\\r')\n",
    "        \n",
    "    counts = {} \n",
    "    for model in models:\n",
    "        counts[model.name] = {'true_positives':0, 'predicted_positives':0, 'possible_positives':0}\n",
    "    \n",
    "    for testIndex in range(splitIndex, numberOfFiles):\n",
    "        dataX, dataY = load_data(datasetFolder,'datasetX_{}.csv'.format(listOfFiles[testIndex]), 'datasetY_{}.csv'.format(listOfFiles[testIndex]))\n",
    "        for model in models:\n",
    "            predY=model.predict_on_batch(dataX)\n",
    "            true_positives, predicted_positives, possible_positives = countPredictions(dataY, predY)\n",
    "            counts[model.name]['true_positives'] += true_positives\n",
    "            counts[model.name]['predicted_positives'] += predicted_positives\n",
    "            counts[model.name]['possible_positives'] += possible_positives\n",
    "            print ('Testing for file {} / {} : datasetX/Y_{} - true +ve:{}  pred +ve:{} possible +ve:{}'.format(testIndex+1, numberOfFiles, listOfFiles[testIndex], true_positives,predicted_positives,possible_positives), end='\\r')\n",
    "    \n",
    "    for model in models:\n",
    "        count = counts[model.name]\n",
    "        precision = (count['true_positives'])/(count['predicted_positives']+0.0001)\n",
    "        recall = (count['true_positives'])/(count['possible_positives']+0.0001)\n",
    "        f1score = 2.0 * precision * recall / (precision+recall+0.0001)\n",
    "        print(' - Model = {} \\t f1-score = {:.4f}\\t precision = {:.4f} \\t recall = {:.4f}'.format(model.name, f1score, precision, recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "==================================================\n",
    "# all4Final_2x10\n",
    "==================================================\n",
    "1. Eon 1/25\n",
    " - Model = Deep Model (5 Dense Layers) \t f1-score = 0.4251\t precision = 0.7243 \t recall = 0.3009\n",
    " - Model = Simple Logistic Regression \t f1-score = 0.3513\t precision = 0.8005 \t recall = 0.2251\n",
    "2. Eon 2/25\n",
    " - Model = Deep Model (5 Dense Layers) \t f1-score = 0.4120\t precision = 0.7618 \t recall = 0.2824\n",
    " - Model = Simple Logistic Regression \t f1-score = 0.3787\t precision = 0.7964 \t recall = 0.2485\n",
    "3. Eon 3/25\n",
    " - Model = Deep Model (5 Dense Layers) \t f1-score = 0.4076\t precision = 0.7667 \t recall = 0.2776\n",
    " - Model = Simple Logistic Regression \t f1-score = 0.3857\t precision = 0.7945 \t recall = 0.2547\n",
    "4. Eon 4/25\n",
    " - Model = Deep Model (5 Dense Layers) \t f1-score = 0.4153\t precision = 0.7620 \t recall = 0.2855\n",
    " - Model = Simple Logistic Regression \t f1-score = 0.3895\t precision = 0.7941 \t recall = 0.2580\n",
    "5. Eon 5/25\n",
    " - Model = Deep Model (5 Dense Layers) \t f1-score = 0.4304\t precision = 0.7418 \t recall = 0.3032\n",
    " - Model = Simple Logistic Regression \t f1-score = 0.3927\t precision = 0.7944 \t recall = 0.2608\n",
    "6. Eon 6/25\n",
    " - Model = Deep Model (5 Dense Layers) \t f1-score = 0.4206\t precision = 0.7757 \t recall = 0.2885\n",
    " - Model = Simple Logistic Regression \t f1-score = 0.3939\t precision = 0.7944 \t recall = 0.2619\n",
    "7. Eon 7/25\n",
    " - Model = Deep Model (5 Dense Layers) \t f1-score = 0.4234\t precision = 0.7495 \t recall = 0.2951\n",
    " - Model = Simple Logistic Regression \t f1-score = 0.3956\t precision = 0.7930 \t recall = 0.2636\n",
    "8. Eon 8/25\n",
    " - Model = Deep Model (5 Dense Layers) \t f1-score = 0.4046\t precision = 0.7662 \t recall = 0.2749\n",
    " - Model = Simple Logistic Regression \t f1-score = 0.3965\t precision = 0.7932 \t recall = 0.2644\n",
    "9. Eon 9/25\n",
    " - Model = Deep Model (5 Dense Layers) \t f1-score = 0.4103\t precision = 0.7660 \t recall = 0.2802\n",
    " - Model = Simple Logistic Regression \t f1-score = 0.3963\t precision = 0.7940 \t recall = 0.2641\n",
    "10. Eon 10/25\n",
    " - Model = Deep Model (5 Dense Layers) \t f1-score = 0.4112\t precision = 0.7831 \t recall = 0.2789\n",
    " - Model = Simple Logistic Regression \t f1-score = 0.3965\t precision = 0.7941 \t recall = 0.2643\n",
    "11. Eon 11/25\n",
    " - Model = Deep Model (5 Dense Layers) \t f1-score = 0.4118\t precision = 0.7681 \t recall = 0.2813\n",
    " - Model = Simple Logistic Regression \t f1-score = 0.3970\t precision = 0.7942 \t recall = 0.2647\n",
    "12. Eon 12/25\n",
    " - Model = Deep Model (5 Dense Layers) \t f1-score = 0.3999\t precision = 0.7808 \t recall = 0.2688\n",
    " - Model = Simple Logistic Regression \t f1-score = 0.3977\t precision = 0.7939 \t recall = 0.2653\n",
    "13. Eon 13/25\n",
    " - Model = Deep Model (5 Dense Layers) \t f1-score = 0.4087\t precision = 0.7533 \t recall = 0.2804\n",
    " - Model = Simple Logistic Regression \t f1-score = 0.3989\t precision = 0.7920 \t recall = 0.2666"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
