{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Nanodegree\n",
    "## Deep Learning\n",
    "## Project: Build a Digit Recognition Program\n",
    "\n",
    "In this notebook, a template is provided for you to implement your functionality in stages which is required to successfully complete this project. If additional code is required that cannot be included in the notebook, be sure that the Python code is successfully imported and included in your submission, if necessary. Sections that begin with **'Implementation'** in the header indicate where you should begin your implementation for your project. Note that some sections of implementation are optional, and will be marked with **'Optional'** in the header.\n",
    "\n",
    "In addition to implementing code, there will be questions that you must answer which relate to the project and your implementation. Each section where you will answer a question is preceded by a **'Question'** header. Carefully read each question and provide thorough answers in the following text boxes that begin with **'Answer:'**. Your project submission will be evaluated based on your answers to each of the questions and the implementation you provide.\n",
    "\n",
    ">**Note:** Code and Markdown cells can be executed using the **Shift + Enter** keyboard shortcut. In addition, Markdown cells can be edited by typically double-clicking the cell to enter edit mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Step 1: Design and Test a Model Architecture\n",
    "Design and implement a deep learning model that learns to recognize sequences of digits. Train the model using synthetic data generated by concatenating character images from [notMNIST](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html) or [MNIST](http://yann.lecun.com/exdb/mnist/). To produce a synthetic sequence of digits for testing, you can for example limit yourself to sequences up to five digits, and use five classifiers on top of your deep network. You would have to incorporate an additional ‘blank’ character to account for shorter number sequences.\n",
    "\n",
    "There are various aspects to consider when thinking about this problem:\n",
    "- Your model can be derived from a deep neural net or a convolutional network.\n",
    "- You could experiment sharing or not the weights between the softmax classifiers.\n",
    "- You can also use a recurrent network in your deep neural net to replace the classification layers and directly emit the sequence of digits one-at-a-time.\n",
    "\n",
    "Here is an example of a [published baseline model on this problem](http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/42241.pdf). ([video](https://www.youtube.com/watch?v=vGPI_JvLoN0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "Use the code cell (or multiple code cells, if necessary) to implement the first step of your project. Once you have completed your implementation and are satisfied with the results, be sure to thoroughly answer the questions that follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------\n",
    "# First, we load all the data from the NotMNIST dataset\n",
    "# -------------------------------------------------\n",
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import math\n",
    "import random\n",
    "    \n",
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)\n",
    "  \n",
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)\n",
    "# -------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------\n",
    "# Let's define some helpful functions\n",
    "# -------------------------------------------------\n",
    "    \n",
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "def oneHot(num, length):\n",
    "    arr = np.zeros(length)\n",
    "    arr[num-1]=1.0\n",
    "    return arr\n",
    "\n",
    "newSize=32\n",
    "imgSize=54\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Then, let's define the functions for returning scale, crop and rotation variants for each image\n",
    "# -------------------------------------------------\n",
    "# input is x by y array\n",
    "def rotate(pixels, newSize, angle):\n",
    "    size = pixels.shape[0]\n",
    "    newPixels = np.full([newSize, newSize], -0.5)\n",
    "    startPos = (newSize-size)//2\n",
    "    for i in range(0,size):\n",
    "        for j in range(0,size):\n",
    "            newPixels[startPos+i][startPos+j] = pixels[i][j]\n",
    "    newPixels = newPixels + 0.5\n",
    "    cols = newSize\n",
    "    rows = newSize\n",
    "    M = cv2.getRotationMatrix2D((cols/2,rows/2),angle, 1)\n",
    "    rotated = cv2.warpAffine(newPixels,M,(cols,rows))\n",
    "    rotated = rotated - 0.5\n",
    "    return rotated        \n",
    "\n",
    "def translate(image, widthTranslate, heightTranslate):\n",
    "    newPixels = image+0.5\n",
    "    M = np.float32([[1,0,widthTranslate],[0,1,heightTranslate]])\n",
    "    newPixels = cv2.warpAffine(newPixels,M,(image.shape[1],image.shape[0]))\n",
    "    return newPixels-0.5\n",
    "\n",
    "# clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(4,4))\n",
    "# def correct(image):\n",
    "#     newPixels = image+0.5\n",
    "# #     kernel_sharpen_1 = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])\n",
    "# #     newPixels = cv2.filter2D(newPixels, -1, kernel_sharpen_1)\n",
    "#     newPixels = cv2.equalizeHist(newPixels)    \n",
    "#     return newPixels-0.5\n",
    "\n",
    "def getVariant(digits, toSize):\n",
    "    angles = [330, 345,0,15,30]\n",
    "    angle = random.choice(angles)\n",
    "    numberOfDigits = digits.shape[0]\n",
    "    variant=np.full([newSize, newSize*numberOfDigits], -0.5) \n",
    "    for num, digit in enumerate(digits):\n",
    "        digit=rotate(digit, newSize, angle)\n",
    "        for i in range(0,newSize):\n",
    "            for j in range(0,newSize):\n",
    "                if(digit[i][j]>-0.5):\n",
    "                    variant[i][(num*newSize)+j] = digit[i][j]    \n",
    "    #variant = translate(variant, random.randint(-5,5), random.randint(-5,5))\n",
    "    variant = cv2.resize(variant, (toSize, toSize)) \n",
    "    return variant\n",
    "\n",
    "import random\n",
    "    \n",
    "#We make the function to get some variants of the image & test it\n",
    "sampleImage1=train_dataset[random.randint(0,train_dataset.shape[0])]\n",
    "sampleImage1=np.reshape(sampleImage1, [28, 28])\n",
    "sampleImage2=train_dataset[random.randint(0,train_dataset.shape[0])]\n",
    "sampleImage2=np.reshape(sampleImage2, [28, 28])\n",
    "sampleImage3=train_dataset[random.randint(0,train_dataset.shape[0])]\n",
    "sampleImage3=np.reshape(sampleImage3, [28, 28])\n",
    "\n",
    "digits = np.ndarray([3,28,28])\n",
    "digits[0] = np.copy(sampleImage1)\n",
    "digits[1] = np.copy(sampleImage2)\n",
    "digits[2] = np.copy(sampleImage3)\n",
    "variant = getVariant(digits, imgSize)\n",
    "plt.imshow(variant, cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "for i in range(0,5):\n",
    "    print(oneHot(i+1,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Let's build the new training set\n",
    "train_size=50000\n",
    "valid_size=5000\n",
    "test_size=5000\n",
    "import random\n",
    "def createDataset(originalDataSet,dataSize,sizeOfDigitsFromOriginal):\n",
    "    newDataset=np.ndarray([dataSize,imgSize*imgSize])\n",
    "    lengthLabels=np.ndarray([dataSize,5])\n",
    "    digitsLabels = np.zeros([dataSize,5,11])\n",
    "    for i in range(0, dataSize):\n",
    "        numberOfDigits=random.randint(1,5)\n",
    "        digits = np.ndarray([5,28,28])\n",
    "        for j in range(0, 5):\n",
    "            index=random.randint(0,sizeOfDigitsFromOriginal);\n",
    "            randImage=train_dataset[index]\n",
    "            randImage=np.reshape(randImage, [28, 28])\n",
    "            digits[j] = np.copy(randImage)\n",
    "            if j < numberOfDigits:\n",
    "                digitsLabels[i][j][0:10]=train_labels[index]\n",
    "            else:\n",
    "                digitsLabels[i][j][10] = 1\n",
    "            \n",
    "        \n",
    "        #print(digitsLabels[i])\n",
    "        variant = getVariant(digits, imgSize)\n",
    "#         plt.imshow(variant, cmap='gray')\n",
    "#         plt.show()\n",
    "        \n",
    "        newDataset[i]=variant.reshape((imgSize*imgSize))\n",
    "        lengthLabels[i]=oneHot(numberOfDigits,5)\n",
    "        #print(lengthLabels[i])\n",
    "        if (i % 500 == 0):\n",
    "            print('Completed %d of %d' % (i, dataSize))\n",
    "    return newDataset, lengthLabels, digitsLabels\n",
    "import os.path\n",
    "import pickle\n",
    "\n",
    "def maybeLoadData(dataset, size, pickleFile):\n",
    "    import os.path\n",
    "    import pickle\n",
    "    file_path=pickleFile + '.pk'\n",
    "    if(os.path.exists(file_path) is False):\n",
    "        imageData, imageLengths, imageDigits = createDataset(dataset, size, dataset.shape[0]//100)\n",
    "        data = { 'imageData': imageData, 'imageLengths': imageLengths, 'imageDigits': imageDigits}\n",
    "        pickle.dump(data , open( file_path, \"wb\" ))\n",
    "        imageData=imageData.astype(np.float32)\n",
    "        imageLengths=imageLengths.astype(np.float32)\n",
    "        \n",
    "    data = pickle.load( open( file_path, \"rb\" ) );\n",
    "    return data['imageData'], data['imageLengths'], data['imageDigits']\n",
    "\n",
    "print('Creating new Train data from about 2000 digits')\n",
    "newTrainData, newTrainLengths, newTrainDigitLabels = maybeLoadData(train_dataset,train_size, 'synth_train')\n",
    "print(\"Training data images: {}\".format(newTrainData.shape))\n",
    "print(\"              length: {}\".format(newTrainLengths.shape))\n",
    "print(\"              digits: {}\".format(newTrainDigitLabels.shape))\n",
    "\n",
    "\n",
    "print('Creating new Validation data from about 100 digits')\n",
    "newValidationData, newValidationLengths, newValidationDigitLabels = maybeLoadData(valid_dataset,valid_size, 'synth_val')\n",
    "print(\"Validation data images: {}\".format(newValidationData.shape))\n",
    "print(\"                length: {}\".format(newValidationLengths.shape))    \n",
    "print(\"                digits: {}\".format(newValidationDigitLabels.shape))\n",
    "\n",
    "print('Creating new Test data from about 100 digits')\n",
    "newTestData, newTestLengths, newTestDigitLabels = maybeLoadData(test_dataset,test_size, 'synth_test')\n",
    "print(\"Test data images: {}\".format(newTestData.shape))\n",
    "print(\"          length: {}\".format(newTestLengths.shape))  \n",
    "print(\"          digits: {}\".format(newTestDigitLabels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Deep Neural Network Model\n",
    "batch_size = 150\n",
    "image_size=imgSize*imgSize\n",
    "layer1_size = imgSize*imgSize//4\n",
    "layer2_size = 256\n",
    "layer3_size = 32\n",
    "num_labels=5\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(None, image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(None, num_labels))\n",
    "  tf_valid_dataset = tf.constant(newValidationData)\n",
    "  tf_test_dataset = tf.constant(newTestData)\n",
    "  \n",
    "  # Variables.\n",
    "  weights1 = tf.get_variable('W1', shape=[image_size, layer1_size], initializer=tf.contrib.layers.xavier_initializer()) \n",
    "  biases1 = tf.Variable(tf.zeros([layer1_size]))\n",
    "    \n",
    "  weights2 = tf.get_variable('W2', shape=[layer1_size, layer2_size], initializer=tf.contrib.layers.xavier_initializer()) \n",
    "  biases2 = tf.Variable(tf.zeros([layer2_size]))\n",
    "    \n",
    "  weights3 = tf.get_variable('W3', shape=[layer2_size, layer3_size], initializer=tf.contrib.layers.xavier_initializer()) \n",
    "  biases3 = tf.Variable(tf.zeros([layer3_size]))\n",
    "    \n",
    "  weights4 = tf.get_variable('W4', shape=[layer3_size, num_labels], initializer=tf.contrib.layers.xavier_initializer()) \n",
    "  biases4 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "  def model(data):\n",
    "      logits1 = tf.matmul(data, weights1) + biases1\n",
    "      relu1 = tf.nn.relu6(logits1)\n",
    "      logits2 = tf.matmul(relu1, weights2) + biases2\n",
    "      relu2 =tf.nn.relu6(logits2)\n",
    "      logits3 = tf.matmul(relu2, weights3) + biases3\n",
    "      relu3 =tf.nn.relu6(logits3)\n",
    "      logits4 = tf.matmul(relu3, weights4) + biases4\n",
    "      return logits4\n",
    "    \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset)\n",
    "  loss1 = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "  l2Loss = loss1 + ((tf.nn.l2_loss(weights1)+ tf.nn.l2_loss(weights2) + tf.nn.l2_loss(weights3) + tf.nn.l2_loss(weights4))/(2*batch_size))\n",
    "    \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.01).minimize(l2Loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset))\n",
    "\n",
    "num_steps = 15001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (newTrainLength.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = newTrainData[offset:(offset + batch_size), :]\n",
    "        batch_labels = newTrainLength[offset:(offset + batch_size), :]\n",
    "\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, l2Loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "            print('Validation accuracy: %.1f%%' % accuracy(\n",
    "                valid_prediction.eval(), newValidationLength))\n",
    "    predicted_test=test_prediction.eval();\n",
    "    print('Test accuracy: %.1f%%' % accuracy(predicted_test, newTestLength))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #Deep Neural Network Model\n",
    "# batch_size = 50\n",
    "# image_size=newSize*newSize*5\n",
    "# layer1_size = newSize*10\n",
    "# layer2_size = 256\n",
    "# layer3_size = 32\n",
    "# num_labels=5\n",
    "# graph = tf.Graph()\n",
    "# with graph.as_default():\n",
    "\n",
    "#   # Input data. For the training data, we use a placeholder that will be fed\n",
    "#   # at run time with a training minibatch.\n",
    "#   tf_train_dataset = tf.placeholder(tf.float32,\n",
    "#                                     shape=(None, image_size))\n",
    "#   tf_train_length = tf.placeholder(tf.float32, shape=(None, num_labels))\n",
    "#   tf_train_digits = tf.placeholder(tf.float32, shape=(None, num_labels * 10))\n",
    "#   tf_train_allhot = tf.placeholder(tf.float32, shape=(None, num_labels))\n",
    "#   tf_valid_dataset = tf.constant(newValidationData)\n",
    "#   tf_test_dataset = tf.constant(newTestData)\n",
    "  \n",
    "#   # Variables.\n",
    "#   weights1 = tf.get_variable('W1', shape=[image_size, layer1_size], initializer=tf.contrib.layers.xavier_initializer()) \n",
    "#   biases1 = tf.Variable(tf.zeros([layer1_size]))\n",
    "    \n",
    "#   weights2 = tf.get_variable('W2', shape=[layer1_size, layer2_size], initializer=tf.contrib.layers.xavier_initializer()) \n",
    "#   biases2 = tf.Variable(tf.zeros([layer2_size]))\n",
    "    \n",
    "#   weights3 = tf.get_variable('W3', shape=[layer2_size, layer3_size], initializer=tf.contrib.layers.xavier_initializer()) \n",
    "#   biases3 = tf.Variable(tf.zeros([layer3_size]))\n",
    "    \n",
    "#   weights4 = tf.get_variable('W4', shape=[layer3_size, num_labels], initializer=tf.contrib.layers.xavier_initializer()) \n",
    "#   biases4 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "#   weights_d0 = tf.get_variable('W_d0', shape=[layer3_size, 10], initializer=tf.contrib.layers.xavier_initializer()) \n",
    "#   biases_d0 = tf.Variable(tf.zeros([10]))\n",
    "#   weights_d1 = tf.get_variable('W_d1', shape=[layer3_size, 10], initializer=tf.contrib.layers.xavier_initializer()) \n",
    "#   biases_d1 = tf.Variable(tf.zeros([10]))\n",
    "#   weights_d2 = tf.get_variable('W_d2', shape=[layer3_size, 10], initializer=tf.contrib.layers.xavier_initializer()) \n",
    "#   biases_d2 = tf.Variable(tf.zeros([10]))\n",
    "#   weights_d3 = tf.get_variable('W_d3', shape=[layer3_size, 10], initializer=tf.contrib.layers.xavier_initializer()) \n",
    "#   biases_d3 = tf.Variable(tf.zeros([10]))\n",
    "#   weights_d4 = tf.get_variable('W_d4', shape=[layer3_size, 10], initializer=tf.contrib.layers.xavier_initializer()) \n",
    "#   biases_d4 = tf.Variable(tf.zeros([10]))\n",
    "    \n",
    "#   def model(data):\n",
    "#       logits1 = tf.matmul(data, weights1) + biases1\n",
    "#       relu1 = tf.nn.relu6(logits1)\n",
    "#       logits2 = tf.matmul(relu1, weights2) + biases2\n",
    "#       relu2 =tf.nn.relu6(logits2)\n",
    "#       logits3 = tf.matmul(relu2, weights3) + biases3\n",
    "#       relu3 =tf.nn.relu6(logits3)\n",
    "#       logits_l = tf.matmul(relu3, weights4) + biases4\n",
    "#       logits_d0 = tf.matmul(relu3, weights_d0) + biases_d0\n",
    "#       logits_d1 = tf.matmul(relu3, weights_d1) + biases_d1\n",
    "#       logits_d2 = tf.matmul(relu3, weights_d2) + biases_d2\n",
    "#       logits_d3 = tf.matmul(relu3, weights_d3) + biases_d3\n",
    "#       logits_d4 = tf.matmul(relu3, weights_d4) + biases_d4\n",
    "#       return logits_l, logits_d0,logits_d1,logits_d2,logits_d3,logits_d4\n",
    "    \n",
    "#   # Training computation.\n",
    "#   logits_l, logits_d0, logits_d1, logits_d2, logits_d3, logits_d4 = model(tf_train_dataset)\n",
    "#   reshaped=tf.reshape(tf_train_digits,[-1, num_labels, 10])  \n",
    "#   loss_0 = tf.nn.softmax_cross_entropy_with_logits(logits_d0, reshaped[:,0,:])\n",
    "#   loss_1 = tf.nn.softmax_cross_entropy_with_logits(logits_d1, reshaped[:,1,:])\n",
    "#   loss_2 = tf.nn.softmax_cross_entropy_with_logits(logits_d2, reshaped[:,2,:])\n",
    "#   loss_3 = tf.nn.softmax_cross_entropy_with_logits(logits_d3, reshaped[:,3,:])\n",
    "#   loss_4 = tf.nn.softmax_cross_entropy_with_logits(logits_d4, reshaped[:,4,:])\n",
    "#   loss_l = tf.nn.softmax_cross_entropy_with_logits(logits_l, tf_train_length)\n",
    "\n",
    "#   loss_d = tf.pack([loss_0,loss_1,loss_2,loss_3,loss_4],axis=1)\n",
    "#   loss_d = tf.mul(tf_train_allhot,loss_d)\n",
    "#   loss=tf.reduce_mean(loss_d)+tf.reduce_mean(loss_l)\n",
    "\n",
    "#   #l2Loss = loss1 + ((tf.nn.l2_loss(weights1)+ tf.nn.l2_loss(weights2) + tf.nn.l2_loss(weights3) + tf.nn.l2_loss(weights4))/(2*batch_size))\n",
    "    \n",
    "#   # Optimizer.\n",
    "#   optimizer = tf.train.GradientDescentOptimizer(0.01).minimize(loss)\n",
    "  \n",
    "#   # Predictions for the training, validation, and test data.\n",
    "#   train_prediction = tf.nn.softmax(logits_l)\n",
    "#   train_prediction_d0 = tf.nn.softmax(logits_d0)\n",
    "#   logits_l, logits_d0,logits_d1,logits_d2,logits_d3,logits_d4 = model(tf_valid_dataset) \n",
    "#   valid_prediction = tf.nn.softmax(logits_l)\n",
    "#   valid_prediction_d0 = tf.nn.softmax(logits_d0)\n",
    "#   logits_l, logits_d0,logits_d1,logits_d2,logits_d3,logits_d4 = model(tf_test_dataset) \n",
    "#   test_prediction = tf.nn.softmax(logits_l)\n",
    "#   test_prediction_d0 = tf.nn.softmax(logits_d0)\n",
    "\n",
    "# num_steps = 3001\n",
    "\n",
    "# with tf.Session(graph=graph) as session:\n",
    "#     tf.global_variables_initializer().run()\n",
    "#     print(\"Initialized\")\n",
    "#     for step in range(num_steps):\n",
    "#         # Pick an offset within the training data, which has been randomized.\n",
    "#         # Note: we could use better randomization across epochs.\n",
    "#         offset = (step * batch_size) % (newTrainLength.shape[0] - batch_size)\n",
    "#         # Generate a minibatch.\n",
    "#         batch_data = newTrainData[offset:(offset + batch_size), :]\n",
    "#         batch_labels = newTrainLength[offset:(offset + batch_size), :]\n",
    "#         batch_digits = newTrainDigitLabels[offset:(offset + batch_size), :]\n",
    "#         re_batch_digits = batch_digits.reshape((-1, num_labels*10)).astype(np.float32)\n",
    "#         batch_allHot = newAllHotLengths[offset:(offset + batch_size), :]\n",
    "        \n",
    "#         # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "#         # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "#         # and the value is the numpy array to feed to it.\n",
    "#         feed_dict = {tf_train_dataset : batch_data, tf_train_length : batch_labels, tf_train_digits:re_batch_digits, tf_train_allhot:batch_allHot}\n",
    "#         _, l, predictions, predictions_d0 = session.run(\n",
    "#           [optimizer, loss, train_prediction, train_prediction_d0], feed_dict=feed_dict)\n",
    "#         if (step % 500 == 0):\n",
    "#             print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "#             print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "#             print('Minibatch D0 accuracy: %.1f%%' % accuracy(predictions_d0,batch_digits[:,0,:]))\n",
    "#             print('Validation accuracy: %.1f%%' % accuracy(\n",
    "#                 valid_prediction.eval(), newValidationLength))\n",
    "#             print('Validation D0 accuracy: %.1f%%' % accuracy(\n",
    "#                 valid_prediction.eval(), newValidationDigitLabels[:,0,:]))\n",
    "#     predicted_test=test_prediction.eval();\n",
    "#     print('Test accuracy: %.1f%%' % accuracy(predicted_test, newTestLength))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generateJointDataSet(size, original_data, original_length, original_digits):\n",
    "    counters = [0,0,0,0,0]\n",
    "    for i in range(0, size):\n",
    "        pixels = original_data[i]\n",
    "        length = np.argmax(original_length[i])+1\n",
    "        for j in range(0,length):\n",
    "            counters[j] = counters[j]+1\n",
    "    \n",
    "    print ('Length of the data for each position: ', counters)\n",
    "    \n",
    "    new_data_p0 = np.ndarray([counters[0], image_size]).astype(np.float32)\n",
    "    new_labels_p0 = np.ndarray([counters[0], 10]).astype(np.float32)\n",
    "    new_data_p1 = np.ndarray([counters[1], image_size]).astype(np.float32)\n",
    "    new_labels_p1 = np.ndarray([counters[1], 10]).astype(np.float32)\n",
    "    new_data_p2 = np.ndarray([counters[2], image_size]).astype(np.float32)\n",
    "    new_labels_p2 = np.ndarray([counters[2], 10]).astype(np.float32)\n",
    "    new_data_p3 = np.ndarray([counters[3], image_size]).astype(np.float32)\n",
    "    new_labels_p3 = np.ndarray([counters[3], 10]).astype(np.float32)\n",
    "    new_data_p4 = np.ndarray([counters[4], image_size]).astype(np.float32)\n",
    "    new_labels_p4 = np.ndarray([counters[4], 10]).astype(np.float32)\n",
    "    \n",
    "    counters=[0,0,0,0,0]\n",
    "    new_map=np.full([size,5],-1)\n",
    "    for i in range(0, size):\n",
    "        pixels = original_data[i]\n",
    "        length = np.argmax(original_length[i])+1\n",
    "        for j in range(0, length):\n",
    "            digit=original_digits[i][j]\n",
    "            if j==0:\n",
    "                new_data_p0[counters[j]]=pixels\n",
    "                new_labels_p0[counters[j]]=digit\n",
    "            if j==1:\n",
    "                new_data_p1[counters[j]]=pixels\n",
    "                new_labels_p1[counters[j]]=digit\n",
    "            if j==2:\n",
    "                new_data_p2[counters[j]]=pixels\n",
    "                new_labels_p2[counters[j]]=digit\n",
    "            if j==3:\n",
    "                new_data_p3[counters[j]]=pixels\n",
    "                new_labels_p3[counters[j]]=digit\n",
    "            if j==4:\n",
    "                new_data_p4[counters[j]]=pixels\n",
    "                new_labels_p4[counters[j]]=digit\n",
    "            new_map[i][j] = int(counters[j])\n",
    "            counters[j] = int(counters[j])+1\n",
    "        if (i % 500 == 0):\n",
    "            print('Completed %d of %d' % (i, size))\n",
    "    return new_data_p0,new_labels_p0,new_data_p1,new_labels_p1,new_data_p2,new_labels_p2,new_data_p3,new_labels_p3,new_data_p4,new_labels_p4,new_map\n",
    "\n",
    "joined_train_data_size = newTrainData.shape[0]\n",
    "joined_validation_data_size = newValidationData.shape[0]\n",
    "joined_test_data_size=newTestData.shape[0]\n",
    "print('Creating joint training data from ground truth')\n",
    "jointTrainData0, jointTrainLabels0, jointTrainData1, jointTrainLabels1, jointTrainData2, jointTrainLabels2, jointTrainData3, jointTrainLabels3, jointTrainData4, jointTrainLabels4, jointTrainDataMap = generateJointDataSet(joined_train_data_size, newTrainData, newTrainLength,newTrainDigitLabels)\n",
    "print('Creating joint validation data from ground truth')\n",
    "jointValidationData0, jointValidationLabels0, jointValidationData1, jointValidationLabels1, jointValidationData2, jointValidationLabels2, jointValidationData3, jointValidationLabels3, jointValidationData4, jointValidationLabels4, jointValidationDataMap = generateJointDataSet(joined_validation_data_size, newValidationData, newValidationLength,newValidationDigitLabels)\n",
    "print('Creating joint test data from prediction')\n",
    "jointTestData0, jointTestLabels0, jointTestData1, jointTestLabels1, jointTestData2, jointTestLabels2, jointTestData3, jointTestLabels3, jointTestData4, jointTestLabels4, jointTestDataMap = generateJointDataSet(joined_test_data_size, newTestData, predicted_test,newTestDigitLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def learnAndPredict(num_steps,jointTrainData,jointTrainLabels,jointValidationData,jointValidationLabels,jointTestData,jointTestLabels):\n",
    "    batch_size = 250\n",
    "    input_size=imgSize*imgSize\n",
    "    layer1_size = input_size//4\n",
    "    layer2_size = 512\n",
    "    layer3_size = 256\n",
    "    \n",
    "   \n",
    "    num_digits = 10\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "\n",
    "      # Input data. For the training data, we use a placeholder that will be fed\n",
    "      # at run time with a training minibatch.\n",
    "      tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                        shape=(None, input_size))\n",
    "      tf_train_labels = tf.placeholder(tf.float32, shape=(None, num_digits))\n",
    "      tf_valid_dataset = tf.constant(jointValidationData)\n",
    "      tf_test_dataset = tf.constant(jointTestData)\n",
    "\n",
    "      # Variables.\n",
    "      weights1 = tf.get_variable('W1', shape=[input_size, layer1_size], initializer=tf.contrib.layers.xavier_initializer()) \n",
    "      biases1 = tf.Variable(tf.zeros([layer1_size]))\n",
    "\n",
    "      weights2 = tf.get_variable('W2', shape=[layer1_size, layer2_size], initializer=tf.contrib.layers.xavier_initializer()) \n",
    "      biases2 = tf.Variable(tf.zeros([layer2_size]))\n",
    "\n",
    "      weights3 = tf.get_variable('W3', shape=[layer2_size, layer3_size], initializer=tf.contrib.layers.xavier_initializer()) \n",
    "      biases3 = tf.Variable(tf.zeros([layer3_size]))\n",
    "\n",
    "      weights4 = tf.get_variable('W4', shape=[layer3_size, num_digits], initializer=tf.contrib.layers.xavier_initializer()) \n",
    "      biases4 = tf.Variable(tf.zeros([num_digits]))\n",
    "\n",
    "     \n",
    "\n",
    "      def model(data):\n",
    "          logits1 = tf.matmul(data, weights1) + biases1\n",
    "          relu1 = tf.nn.relu6(logits1)\n",
    "          logits2 = tf.matmul(relu1, weights2) + biases2\n",
    "          relu2 =tf.nn.relu6(logits2)\n",
    "          logits3 = tf.matmul(relu2, weights3) + biases3\n",
    "          relu3 =tf.nn.relu6(logits3)\n",
    "          logits4 = tf.matmul(relu3, weights4) + biases4\n",
    "          return logits4\n",
    "\n",
    "      # Training computation.\n",
    "      logits = model(tf_train_dataset)\n",
    "      loss1 = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "      l2Loss = loss1 + ((tf.nn.l2_loss(weights1)+ tf.nn.l2_loss(weights2) + tf.nn.l2_loss(weights3) + tf.nn.l2_loss(weights4) )/(2*batch_size))\n",
    "\n",
    "      # Optimizer.\n",
    "      optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(l2Loss)\n",
    "\n",
    "      # Predictions for the training, validation, and test data.\n",
    "      train_prediction = tf.nn.softmax(logits)\n",
    "      valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "      test_prediction = tf.nn.softmax(model(tf_test_dataset))\n",
    "\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        print(\"Initialized\")\n",
    "        for step in range(num_steps):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            # Note: we could use better randomization across epochs.\n",
    "            offset = (step * batch_size) % (jointTrainLabels.shape[0] - batch_size)\n",
    "            # Generate a minibatch.\n",
    "            batch_data = jointTrainData[offset:(offset + batch_size), :]\n",
    "            batch_labels = jointTrainLabels[offset:(offset + batch_size), :]\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "            _, l, predictions = session.run(\n",
    "              [optimizer, l2Loss, train_prediction], feed_dict=feed_dict)\n",
    "            if (step % 500 == 0):\n",
    "                print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "                print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "                print('Validation accuracy: %.1f%%' % accuracy(\n",
    "                    valid_prediction.eval(), jointValidationLabels))\n",
    "        predicted_digits=test_prediction.eval();\n",
    "        print('Test accuracy: %.1f%%' % accuracy(predicted_digits, jointTestLabels))\n",
    "    return predicted_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def learnAndPredict(num_steps,jointTrainData,jointTrainLabels,jointValidationData,jointValidationLabels,jointTestData,jointTestLabels):\n",
    "#     batch_size = 256\n",
    "#     patch_size_1 = 6\n",
    "#     depth_1 = 6\n",
    "#     patch_size_2 = 6\n",
    "#     depth_2 = 16\n",
    "#     patch_size_3 = 6\n",
    "#     depth_3 = 64\n",
    "#     num_hidden = 128\n",
    "#     num_channels = 1\n",
    "#     num_digits = 10\n",
    "\n",
    "#     jointTrainData=jointTrainData.reshape((-1, imgSize, imgSize, num_channels)).astype(np.float32)\n",
    "#     jointValidationData=jointValidationData.reshape((-1, imgSize, imgSize, num_channels)).astype(np.float32)\n",
    "#     jointTestData=jointTestData.reshape((-1, imgSize, imgSize, num_channels)).astype(np.float32)\n",
    "    \n",
    "#     graph = tf.Graph()\n",
    "\n",
    "#     with graph.as_default():\n",
    "\n",
    "#         # Input data.\n",
    "#         tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, imgSize, imgSize, num_channels))\n",
    "#         tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_digits))\n",
    "#         tf_valid_dataset = tf.constant(jointValidationData)\n",
    "#         tf_test_dataset = tf.constant(jointTestData)\n",
    "\n",
    "#         # Variables.\n",
    "#         layer1_weights = tf.get_variable('W1', shape=[patch_size_1 , patch_size_1, num_channels, depth_1], initializer=tf.contrib.layers.xavier_initializer()) \n",
    "#         layer1_biases = tf.Variable(tf.zeros([depth_1]))\n",
    "#         layer2_weights = tf.get_variable('W2', shape=[patch_size_2 , patch_size_2, depth_1, depth_2], initializer=tf.contrib.layers.xavier_initializer()) \n",
    "#         layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth_2]))\n",
    "#         layer3_weights = tf.get_variable('W3', shape=[patch_size_3 , patch_size_3, depth_2, depth_3], initializer=tf.contrib.layers.xavier_initializer()) \n",
    "#         layer3_biases = tf.Variable(tf.constant(1.0, shape=[depth_3]))\n",
    "#         layer4_weights = tf.get_variable('W4', shape=[imgSize // 4 * (imgSize) // 4 * depth_3, num_hidden], initializer=tf.contrib.layers.xavier_initializer()) \n",
    "#         layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "#         layer5_weights = tf.get_variable('W5', shape=[num_hidden , num_digits], initializer=tf.contrib.layers.xavier_initializer()) \n",
    "#         layer5_biases = tf.Variable(tf.constant(1.0, shape=[num_digits]))\n",
    "\n",
    "\n",
    "#         # Model.\n",
    "#         def model(data, train):\n",
    "#             conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "#             hidden = tf.nn.relu(conv + layer1_biases)\n",
    "# #             if(train==1):\n",
    "# #                 hidden = tf.nn.dropout(hidden,0.5)\n",
    "#             pool = tf.nn.max_pool(hidden, [1, 2, 2, 1], [1, 2, 2, 1], padding='VALID')\n",
    "#             conv = tf.nn.conv2d(pool, layer2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "#             hidden = tf.nn.relu(conv + layer2_biases)\n",
    "# #             if(train==1):\n",
    "# #                 hidden = tf.nn.dropout(hidden,0.5)\n",
    "#             pool = tf.nn.max_pool(hidden, [1, 2, 2, 1], [1, 2, 2, 1], padding='VALID')\n",
    "#             conv = tf.nn.conv2d(pool, layer3_weights, [1, 1, 1, 1], padding='SAME')\n",
    "#             hidden = tf.nn.relu(conv + layer3_biases)\n",
    "#             shape = hidden.get_shape().as_list()\n",
    "#             reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "#             hidden = tf.nn.relu(tf.matmul(reshape, layer4_weights) + layer4_biases)\n",
    "#             logits = tf.matmul(hidden, layer5_weights) + layer5_biases\n",
    "#             return logits\n",
    "\n",
    "#         # Training computation.\n",
    "#         logits = model(tf_train_dataset,1)\n",
    "#         loss1 = tf.reduce_mean(\n",
    "#             tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "#         l2Loss = loss1 + ((tf.nn.l2_loss(layer1_weights)+ tf.nn.l2_loss(layer2_weights) + tf.nn.l2_loss(layer3_weights) + tf.nn.l2_loss(layer4_weights) + tf.nn.l2_loss(layer5_weights) )/(2*batch_size))\n",
    "\n",
    "#         # Optimizer.\n",
    "#         optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(l2Loss)\n",
    "\n",
    "#         # Predictions for the training, validation, and test data.\n",
    "#         train_prediction = tf.nn.softmax(logits)\n",
    "#         valid_prediction = tf.nn.softmax(model(tf_valid_dataset,0))\n",
    "#         test_prediction = tf.nn.softmax(model(tf_test_dataset,0))\n",
    "\n",
    "#         with tf.Session(graph=graph) as session:\n",
    "#             tf.global_variables_initializer().run()\n",
    "#             print(\"Initialized\")\n",
    "#             for step in range(num_steps):\n",
    "#                 # Pick an offset within the training data, which has been randomized.\n",
    "#                 # Note: we could use better randomization across epochs.\n",
    "#                 offset = (step * batch_size) % (jointTrainLabels.shape[0] - batch_size)\n",
    "#                 # Generate a minibatch.\n",
    "#                 batch_data = jointTrainData[offset:(offset + batch_size), :,:,:]\n",
    "#                 batch_labels = jointTrainLabels[offset:(offset + batch_size), :]\n",
    "#                 # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "#                 # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "#                 # and the value is the numpy array to feed to it.\n",
    "#                 feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "#                 _, l, predictions = session.run(\n",
    "#                   [optimizer, l2Loss, train_prediction], feed_dict=feed_dict)\n",
    "#                 if (step % 200 == 0):\n",
    "#                     print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "#                     print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "#                     print('Validation accuracy: %.1f%%' % accuracy(\n",
    "#                         valid_prediction.eval(), jointValidationLabels))\n",
    "#             predicted_digits=test_prediction.eval();\n",
    "#             print('Test accuracy: %.1f%%' % accuracy(predicted_digits, jointTestLabels))\n",
    "#     return predicted_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictedDigits4=learnAndPredict(20001,jointTrainData4,jointTrainLabels4,jointValidationData4, jointValidationLabels4,jointTestData4,jointTestLabels4)\n",
    "predictedDigits3=learnAndPredict(20001,jointTrainData3,jointTrainLabels3,jointValidationData3, jointValidationLabels3,jointTestData3,jointTestLabels3)\n",
    "predictedDigits2=learnAndPredict(20001,jointTrainData2,jointTrainLabels2,jointValidationData2, jointValidationLabels2,jointTestData2,jointTestLabels2)\n",
    "predictedDigits1=learnAndPredict(20001,jointTrainData1,jointTrainLabels1,jointValidationData1, jointValidationLabels1,jointTestData1,jointTestLabels1)\n",
    "predictedDigits0=learnAndPredict(20001,jointTrainData0,jointTrainLabels0,jointValidationData0, jointValidationLabels0,jointTestData0,jointTestLabels0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "_What approach did you take in coming up with a solution to this problem?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "_What does your final architecture look like? (Type of model, layers, sizes, connectivity, etc.)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "_How did you train your model? How did you generate your synthetic dataset?_ Include examples of images from the synthetic data you constructed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Step 2: Train a Model on a Realistic Dataset\n",
    "Once you have settled on a good architecture, you can train your model on real data. In particular, the [Street View House Numbers (SVHN)](http://ufldl.stanford.edu/housenumbers/) dataset is a good large-scale dataset collected from house numbers in Google Street View. Training on this more challenging dataset, where the digits are not neatly lined-up and have various skews, fonts and colors, likely means you have to do some hyperparameter exploration to perform well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "Use the code cell (or multiple code cells, if necessary) to implement the first step of your project. Once you have completed your implementation and are satisfied with the results, be sure to thoroughly answer the questions that follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data\n",
      "Training data images: (33402, 54, 54, 1)\n",
      "              length: (33402, 5)\n",
      "              digits: (33402, 5, 11)\n",
      "Loaded 0 out of 100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:133: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 500 out of 100000\n",
      "Loaded 1000 out of 100000\n",
      "Loaded 1500 out of 100000\n",
      "Loaded 2000 out of 100000\n",
      "Loaded 2500 out of 100000\n",
      "Loaded 3000 out of 100000\n",
      "Loaded 3500 out of 100000\n",
      "Loaded 4000 out of 100000\n",
      "Loaded 4500 out of 100000\n",
      "Loaded 5000 out of 100000\n",
      "Loaded 5500 out of 100000\n",
      "Loaded 6000 out of 100000\n",
      "Loaded 6500 out of 100000\n",
      "Loaded 7000 out of 100000\n",
      "Loaded 7500 out of 100000\n",
      "Loaded 8000 out of 100000\n",
      "Loaded 8500 out of 100000\n",
      "Loaded 9000 out of 100000\n",
      "Loaded 9500 out of 100000\n",
      "Loaded 10000 out of 100000\n",
      "Loaded 10500 out of 100000\n",
      "Loaded 11000 out of 100000\n",
      "Loaded 11500 out of 100000\n",
      "Loaded 12000 out of 100000\n",
      "Loaded 12500 out of 100000\n",
      "Loaded 13000 out of 100000\n",
      "Loaded 13500 out of 100000\n",
      "Loaded 14000 out of 100000\n",
      "Loaded 14500 out of 100000\n",
      "Loaded 15000 out of 100000\n",
      "Loaded 15500 out of 100000\n",
      "Loaded 16000 out of 100000\n",
      "Loaded 16500 out of 100000\n",
      "Loaded 17000 out of 100000\n",
      "Loaded 17500 out of 100000\n",
      "Loaded 18000 out of 100000\n",
      "Loaded 18500 out of 100000\n",
      "Loaded 19000 out of 100000\n",
      "Loaded 19500 out of 100000\n",
      "Loaded 20000 out of 100000\n",
      "Loaded 20500 out of 100000\n",
      "Loaded 21000 out of 100000\n",
      "Loaded 21500 out of 100000\n",
      "Loaded 22000 out of 100000\n",
      "Loaded 22500 out of 100000\n",
      "Loaded 23000 out of 100000\n",
      "Loaded 23500 out of 100000\n",
      "Loaded 24000 out of 100000\n",
      "Loaded 24500 out of 100000\n",
      "Loaded 25000 out of 100000\n",
      "Loaded 25500 out of 100000\n",
      "Loaded 26000 out of 100000\n",
      "Loaded 26500 out of 100000\n",
      "Loaded 27000 out of 100000\n",
      "Loaded 27500 out of 100000\n",
      "Loaded 28000 out of 100000\n",
      "Loaded 28500 out of 100000\n",
      "Loaded 29000 out of 100000\n",
      "Loaded 29500 out of 100000\n",
      "Loaded 30000 out of 100000\n",
      "Loaded 30500 out of 100000\n",
      "Loaded 31000 out of 100000\n",
      "Loaded 31500 out of 100000\n",
      "Loaded 32000 out of 100000\n",
      "Loaded 32500 out of 100000\n",
      "Loaded 33000 out of 100000\n",
      "Loaded 33500 out of 100000\n",
      "Loaded 34000 out of 100000\n",
      "Loaded 34500 out of 100000\n",
      "Loaded 35000 out of 100000\n",
      "Loaded 35500 out of 100000\n",
      "Loaded 36000 out of 100000\n",
      "Loaded 36500 out of 100000\n",
      "Loaded 37000 out of 100000\n",
      "Loaded 37500 out of 100000\n",
      "Loaded 38000 out of 100000\n",
      "Loaded 38500 out of 100000\n",
      "Loaded 39000 out of 100000\n",
      "Loaded 39500 out of 100000\n",
      "Loaded 40000 out of 100000\n",
      "Loaded 40500 out of 100000\n",
      "Loaded 41000 out of 100000\n",
      "Loaded 41500 out of 100000\n",
      "Loaded 42000 out of 100000\n",
      "Loaded 42500 out of 100000\n",
      "Loaded 43000 out of 100000\n",
      "Loaded 43500 out of 100000\n",
      "Loaded 44000 out of 100000\n",
      "Loaded 44500 out of 100000\n",
      "Loaded 45000 out of 100000\n",
      "Loaded 45500 out of 100000\n",
      "Loaded 46000 out of 100000\n",
      "Loaded 46500 out of 100000\n",
      "Loaded 47000 out of 100000\n",
      "Loaded 47500 out of 100000\n",
      "Loaded 48000 out of 100000\n",
      "Loaded 48500 out of 100000\n",
      "Loaded 49000 out of 100000\n",
      "Loaded 49500 out of 100000\n",
      "Loaded 50000 out of 100000\n",
      "Loaded 50500 out of 100000\n",
      "Loaded 51000 out of 100000\n",
      "Loaded 51500 out of 100000\n",
      "Loaded 52000 out of 100000\n",
      "Loaded 52500 out of 100000\n",
      "Loaded 53000 out of 100000\n",
      "Loaded 53500 out of 100000\n",
      "Loaded 54000 out of 100000\n",
      "Loaded 54500 out of 100000\n",
      "Loaded 55000 out of 100000\n",
      "Loaded 55500 out of 100000\n",
      "Loaded 56000 out of 100000\n",
      "Loaded 56500 out of 100000\n",
      "Loaded 57000 out of 100000\n",
      "Loaded 57500 out of 100000\n",
      "Loaded 58000 out of 100000\n",
      "Loaded 58500 out of 100000\n",
      "Loaded 59000 out of 100000\n",
      "Loaded 59500 out of 100000\n",
      "Loaded 60000 out of 100000\n",
      "Loaded 60500 out of 100000\n",
      "Loaded 61000 out of 100000\n",
      "Loaded 61500 out of 100000\n",
      "Loaded 62000 out of 100000\n",
      "Loaded 62500 out of 100000\n",
      "Loaded 63000 out of 100000\n",
      "Loaded 63500 out of 100000\n",
      "Loaded 64000 out of 100000\n",
      "Loaded 64500 out of 100000\n",
      "Loaded 65000 out of 100000\n",
      "Loaded 65500 out of 100000\n",
      "Loaded 66000 out of 100000\n",
      "Loaded 66500 out of 100000\n",
      "Loaded 67000 out of 100000\n",
      "Loaded 67500 out of 100000\n",
      "Loaded 68000 out of 100000\n",
      "Loaded 68500 out of 100000\n",
      "Loaded 69000 out of 100000\n",
      "Loaded 69500 out of 100000\n",
      "Loaded 70000 out of 100000\n",
      "Loaded 70500 out of 100000\n",
      "Loaded 71000 out of 100000\n",
      "Loaded 71500 out of 100000\n",
      "Loaded 72000 out of 100000\n",
      "Loaded 72500 out of 100000\n",
      "Loaded 73000 out of 100000\n",
      "Loaded 73500 out of 100000\n",
      "Loaded 74000 out of 100000\n",
      "Loaded 74500 out of 100000\n",
      "Loaded 75000 out of 100000\n",
      "Loaded 75500 out of 100000\n",
      "Loaded 76000 out of 100000\n",
      "Loaded 76500 out of 100000\n",
      "Loaded 77000 out of 100000\n",
      "Loaded 77500 out of 100000\n",
      "Loaded 78000 out of 100000\n",
      "Loaded 78500 out of 100000\n",
      "Loaded 79000 out of 100000\n",
      "Loaded 79500 out of 100000\n",
      "Loaded 80000 out of 100000\n",
      "Loaded 80500 out of 100000\n",
      "Loaded 81000 out of 100000\n",
      "Loaded 81500 out of 100000\n",
      "Loaded 82000 out of 100000\n",
      "Loaded 82500 out of 100000\n",
      "Loaded 83000 out of 100000\n",
      "Loaded 83500 out of 100000\n",
      "Loaded 84000 out of 100000\n",
      "Loaded 84500 out of 100000\n",
      "Loaded 85000 out of 100000\n",
      "Loaded 85500 out of 100000\n",
      "Loaded 86000 out of 100000\n",
      "Loaded 86500 out of 100000\n",
      "Loaded 87000 out of 100000\n",
      "Loaded 87500 out of 100000\n",
      "Loaded 88000 out of 100000\n",
      "Loaded 88500 out of 100000\n",
      "Loaded 89000 out of 100000\n",
      "Loaded 89500 out of 100000\n",
      "Loaded 90000 out of 100000\n",
      "Loaded 90500 out of 100000\n",
      "Loaded 91000 out of 100000\n",
      "Loaded 91500 out of 100000\n",
      "Loaded 92000 out of 100000\n",
      "Loaded 92500 out of 100000\n",
      "Loaded 93000 out of 100000\n",
      "Loaded 93500 out of 100000\n",
      "Loaded 94000 out of 100000\n",
      "Loaded 94500 out of 100000\n",
      "Loaded 95000 out of 100000\n",
      "Loaded 95500 out of 100000\n",
      "Loaded 96000 out of 100000\n",
      "Loaded 96500 out of 100000\n",
      "Loaded 97000 out of 100000\n",
      "Loaded 97500 out of 100000\n",
      "Loaded 98000 out of 100000\n",
      "Loaded 98500 out of 100000\n",
      "Loaded 99000 out of 100000\n",
      "Loaded 99500 out of 100000\n",
      "Extra data    images: (100000, 54, 54, 1)\n",
      "              length: (100000, 5)\n",
      "              digits: (100000, 5, 11)\n",
      "Training data images: (133402, 54, 54, 1)\n",
      "              length: (133402, 5)\n",
      "              digits: (133402, 5, 11)\n",
      "Loading test & validation data\n",
      "Folder test data images: (13068, 54, 54, 1)\n",
      "          length: (13068, 5)\n",
      "          digits: (13068, 5, 11)\n",
      "Validation data images: (6534, 54, 54, 1)\n",
      "                length: (6534, 5)\n",
      "                digits: (6534, 5, 11)\n",
      "Test data images: (6534, 54, 54, 1)\n",
      "          length: (6534, 5)\n",
      "          digits: (6534, 5, 11)\n",
      "Data loaded.\n"
     ]
    }
   ],
   "source": [
    "#Define some useful Functions\n",
    "import h5py\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import math\n",
    "import random\n",
    "\n",
    "#CONSTANTS\n",
    "imgSize = 54\n",
    "\n",
    "def get_box_data(index, hdf5_data):\n",
    "    \"\"\"\n",
    "    get `left, top, width, height` of each picture\n",
    "    :param index:\n",
    "    :param hdf5_data:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    meta_data = dict()\n",
    "    meta_data['height'] = []\n",
    "    meta_data['label'] = []\n",
    "    meta_data['left'] = []\n",
    "    meta_data['top'] = []\n",
    "    meta_data['width'] = []\n",
    "\n",
    "    def print_attrs(name, obj):\n",
    "        vals = []\n",
    "        if obj.shape[0] == 1:\n",
    "            vals.append(obj[0][0])\n",
    "        else:\n",
    "            for k in range(obj.shape[0]):\n",
    "                vals.append(int(hdf5_data[obj[k][0]][0][0]))\n",
    "        meta_data[name] = vals\n",
    "\n",
    "    box = hdf5_data['/digitStruct/bbox'][index]\n",
    "    hdf5_data[box[0]].visititems(print_attrs)\n",
    "    return meta_data\n",
    "\n",
    "def get_name(index, hdf5_data):\n",
    "    name = hdf5_data['/digitStruct/name']\n",
    "    return ''.join([chr(v[0]) for v in hdf5_data[name[index][0]].value])\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "def oneHot(num, length):\n",
    "    arr = np.zeros(length)\n",
    "    arr[num-1]=1\n",
    "    return arr\n",
    "def maybeLoadData(folder, variations):\n",
    "    import os.path\n",
    "    import pickle\n",
    "    file_path=folder + '.pk'\n",
    "    if(os.path.exists(file_path) is False):\n",
    "        imageData, imageLengths, imageDigits = loadData(folder, variations)\n",
    "        data = { 'imageData': imageData, 'imageLengths': imageLengths, 'imageDigits': imageDigits}\n",
    "        out = open( file_path, \"wb\" )\n",
    "        pickle.dump(data , out )\n",
    "        out.close()\n",
    "    data = pickle.load( open( file_path, \"rb\" ) );\n",
    "    return data['imageData'], data['imageLengths'], data['imageDigits']\n",
    "    \n",
    "def loadData(folder, variations):\n",
    "    #First load the data using h5py\n",
    "    f = h5py.File(folder + '/' + 'digitStruct.mat')\n",
    "    #Get the number of images to iterate through them\n",
    "    length = len(f['/digitStruct/name'])\n",
    "    \n",
    "    if(length>100000):\n",
    "        length = 100000;   #MaxLength\n",
    "    \n",
    "    imageData = np.zeros([length, imgSize,imgSize,1]).astype(np.float32)\n",
    "    imageLengths = np.zeros([length, 5]).astype(np.int)\n",
    "    imageDigits = np.zeros([length,5,11]).astype(np.int)\n",
    "    \n",
    "    #Iterate through the images\n",
    "    for i in range(0,length):\n",
    "        if(i%500==0): #In case of error, comment this line\n",
    "            print(\"Loaded {} out of {}\".format(i,length)) \n",
    "        #Read the image\n",
    "        imageFile = folder + '/' + get_name(i,f)\n",
    "        img = cv2.imread(imageFile)\n",
    "\n",
    "        #Read the box data & get the bounding box for all characters (using first and last digit)\n",
    "        boxData=get_box_data(i, f)\n",
    "        \n",
    "        firstTop = int(boxData['top'][0])\n",
    "        firstLeft = int(boxData['left'][0])\n",
    "        firstRight = int(boxData['left'][0]) + int(boxData['width'][0]) \n",
    "        firstBottom = int(boxData['top'][0]) + int(boxData['height'][0])\n",
    "        \n",
    "        l = len(boxData['top'])\n",
    "        lastTop = int(boxData['top'][l-1])\n",
    "        lastLeft = int(boxData['left'][l-1])\n",
    "        lastRight = int(boxData['left'][l-1]) + int(boxData['width'][l-1]) \n",
    "        lastBottom = int(boxData['top'][l-1]) + int(boxData['height'][l-1])\n",
    "        \n",
    "        top = min(firstTop, lastTop)\n",
    "        left = min(firstLeft, lastLeft)\n",
    "        right = max(firstRight, lastRight)\n",
    "        bottom = max(firstBottom, lastBottom)\n",
    "        \n",
    "        height = bottom-top\n",
    "        width = right-left\n",
    "        vertMiddle = (bottom+top)//2\n",
    "        horCenter = (left+right)//2\n",
    "        \n",
    "        if(variations==True):\n",
    "            top = vertMiddle - ((1.3*height)//2)\n",
    "            bottom = vertMiddle + ((1.3*height)//2)\n",
    "            left = horCenter - ((1.3*width)//2)\n",
    "            right = horCenter + ((1.3*width)//2)\n",
    "        \n",
    "        top = max(top, 0)\n",
    "        left = max(left, 0)\n",
    "        right = min(right, img.shape[1])\n",
    "        bottom = min(bottom, img.shape[0])\n",
    "         \n",
    "            #One image has incorrect label length of  6 \n",
    "#         if(len(boxData['label'])>5):\n",
    "#             cv2.imshow('image',img)\n",
    "#             cv2.waitKey(0)\n",
    "\n",
    "        #Check to see the bounding box\n",
    "        #cv2.rectangle(img,(left,top),(right, bottom),(0,255,0),3)\n",
    "\n",
    "        #Extract only the RoI for faster pre-processing\n",
    "        img = img[top:bottom, left:right]\n",
    "        \n",
    "        #Convert to gray scale if in color\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        #Histogram correction\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(6,6))\n",
    "        img = clahe.apply(img)\n",
    "        \n",
    "        #Length of digits \n",
    "        numberOfDigits = len(boxData['label'])\n",
    "        \n",
    "        #Save the original for comparison if needed\n",
    "        #orig = cv2.resize(img,(imgSize, imgSize), interpolation = cv2.INTER_LANCZOS4)\n",
    "                \n",
    "        #Enhance edges\n",
    "#         edge_enhancement_kernel = np.array([[-1,-1,-1,-1,-1],\n",
    "#                              [-1,2,2,2,-1],\n",
    "#                              [-1,2,8,2,-1],\n",
    "#                              [-1,2,2,2,-1],\n",
    "#                              [-1,-1,-1,-1,-1]]) / 8.0\n",
    "        \n",
    "            \n",
    "#         img = cv2.filter2D(img, -1, edge_enhancement_kernel)\n",
    "#         img = cv2.filter2D(img, -1, edge_enhancement_kernel)\n",
    "        \n",
    "        #Resize the image to 64x64\n",
    "        if(variations==True):\n",
    "            img = cv2.resize(img,(64, 64), interpolation = cv2.INTER_LANCZOS4)\n",
    "            leftStart=random.randint(0,9)\n",
    "            topStart=random.randint(0,9)    \n",
    "            img = img[topStart:(topStart+imgSize), leftStart:(leftStart+imgSize)]\n",
    "        else: \n",
    "            img = cv2.resize(img,(imgSize, imgSize), interpolation = cv2.INTER_LANCZOS4)\n",
    "            \n",
    "        #Copy the data\n",
    "        oneImageData = np.resize(img, (imgSize,imgSize,1)).astype(np.float32)\n",
    "        \n",
    "        oneImageData=oneImageData/255.0\n",
    "        oneImageData=oneImageData-0.5\n",
    "        \n",
    "        imageData[i] = oneImageData\n",
    "        first=0\n",
    "        if(numberOfDigits>5):\n",
    "            numberOfDigits=5\n",
    "            print(boxData['label'])\n",
    "            first=1\n",
    "            \n",
    "        imageLengths[i] = oneHot(numberOfDigits,5)\n",
    "        \n",
    "        for k in range(0,5):\n",
    "            if(k<numberOfDigits):\n",
    "                imageDigits[i,k,:]=oneHot(int(boxData['label'][int(k+first)]),11)\n",
    "            else:\n",
    "                imageDigits[i,k,10]=1\n",
    "        \n",
    "            \n",
    "        #Show the original image\n",
    "        #cv2.imshow('image',orig)\n",
    "        #cv2.waitKey(0)\n",
    "        \n",
    "        #Show the processed image\n",
    "        #cv2.imshow('image',img)\n",
    "        #cv2.waitKey(0)\n",
    "        \n",
    "    shuffledIndexes  = np.arange(length)\n",
    "    np.random.shuffle(shuffledIndexes)\n",
    "    \n",
    "    imageData = imageData[shuffledIndexes,:]\n",
    "    imageLengths = imageLengths[shuffledIndexes,:]\n",
    "    imageDigits = imageDigits[shuffledIndexes,:,:]\n",
    "    return imageData,imageLengths, imageDigits\n",
    "        \n",
    "print(\"Loading training data\")    \n",
    "trainImageData, trainImageLengths,trainImageDigits = maybeLoadData('train', True)\n",
    "print(\"Training data images: {}\".format(trainImageData.shape))\n",
    "print(\"              length: {}\".format(trainImageLengths.shape))\n",
    "print(\"              digits: {}\".format(trainImageDigits.shape))\n",
    "extraImageData, extraImageLengths,extraImageDigits = maybeLoadData('extra', True)\n",
    "print(\"Extra data    images: {}\".format(extraImageData.shape))\n",
    "print(\"              length: {}\".format(extraImageLengths.shape))\n",
    "print(\"              digits: {}\".format(extraImageDigits.shape))\n",
    "\n",
    "trainImageData  = np.concatenate((trainImageData, extraImageData))\n",
    "trainImageLengths  = np.concatenate((trainImageLengths, extraImageLengths))\n",
    "trainImageDigits  = np.concatenate((trainImageDigits, extraImageDigits))\n",
    "print(\"Training data images: {}\".format(trainImageData.shape))\n",
    "print(\"              length: {}\".format(trainImageLengths.shape))\n",
    "print(\"              digits: {}\".format(trainImageDigits.shape))\n",
    "\n",
    "\n",
    "print(\"Loading test & validation data\")    \n",
    "folderImageData, folderImageLengths,folderImageDigits = maybeLoadData('test', False)\n",
    "print(\"Folder test data images: {}\".format(folderImageData.shape))\n",
    "print(\"          length: {}\".format(folderImageLengths.shape))\n",
    "print(\"          digits: {}\".format(folderImageDigits.shape))\n",
    "\n",
    "half = len(folderImageData)//2\n",
    "validationImageData = folderImageData[0:half,:]\n",
    "validationImageLengths = folderImageLengths[0:half,:]\n",
    "validationImageDigits= folderImageDigits[0:half,:,:]\n",
    "print(\"Validation data images: {}\".format(validationImageData.shape))\n",
    "print(\"                length: {}\".format(validationImageLengths.shape))    \n",
    "print(\"                digits: {}\".format(validationImageDigits.shape))\n",
    "\n",
    "testImageData = folderImageData[half:,:]\n",
    "testImageLengths = folderImageLengths[half:,:]\n",
    "testImageDigits= folderImageDigits[half:,:,:]\n",
    "print(\"Test data images: {}\".format(testImageData.shape))\n",
    "print(\"          length: {}\".format(testImageLengths.shape))  \n",
    "print(\"          digits: {}\".format(testImageDigits.shape))\n",
    "\n",
    "print(\"Data loaded.\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYaJk8IcwUTL4Q5goGfwhTJQM/hAmyv8HrukXApW8P8sAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13353a310>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 0]\n",
      "[[1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 1]]\n",
      "[1 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 1 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 1]\n",
      "[0 0 0 0 0 0 0 0 0 0 1]\n",
      "[0 0 0 0 0 0 0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "#Confirm Data\n",
    "d=645\n",
    "plt.imshow(testImageData[d].reshape((imgSize,imgSize)), cmap='gray')\n",
    "plt.show()\n",
    "print(testImageLengths[d])\n",
    "print(testImageDigits[d])\n",
    "\n",
    "testDigit0  = testImageDigits[:,0,:]\n",
    "testDigit1  = testImageDigits[:,1,:]\n",
    "testDigit2  = testImageDigits[:,2,:]\n",
    "testDigit3  = testImageDigits[:,3,:]\n",
    "testDigit4  = testImageDigits[:,4,:]\n",
    "\n",
    "print(testDigit0[d])\n",
    "print(testDigit1[d])\n",
    "print(testDigit2[d])\n",
    "print(testDigit3[d])\n",
    "print(testDigit4[d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 83402 samples, validate on 6534 samples\n",
      "Epoch 1/100\n",
      "  450/83402 [..............................] - ETA: 5171s - loss: 48.8434 - Length_loss: 1.3188 - Digit0_loss: 2.2034 - Digit1_loss: 2.4523 - Digit2_loss: 2.0271 - Digit3_loss: 0.7019 - Digit4_loss: 0.4699 - Length_acc: 0.3644 - Digit0_acc: 0.2156 - Digit1_acc: 0.0978 - Digit2_acc: 0.4267 - Digit3_acc: 0.8422 - Digit4_acc: 0.8889     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-343593371332>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0mvalidationDigit3\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mvalidationImageDigits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0mvalidationDigit4\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mvalidationImageDigits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainReshaped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrainImageLengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainDigit0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainDigit1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainDigit2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainDigit3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainDigit4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidationReshaped\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalidationImageLengths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidationDigit0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidationDigit1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidationDigit2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidationDigit3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidationDigit4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'actualModel.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch)\u001b[0m\n\u001b[1;32m   1190\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1192\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, nb_epoch, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m    890\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 892\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    893\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1898\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1899\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 1900\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   1901\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1902\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 964\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    965\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1014\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1015\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1019\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1002\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Activation, Reshape, MaxPooling2D, Input, Flatten, merge, Convolution2D, Dropout, BatchNormalization\n",
    "from keras.regularizers import l2\n",
    "%matplotlib inline\n",
    "\n",
    "batch_size = 50\n",
    "inputSize=imgSize*imgSize\n",
    "num_labels=5\n",
    "epochs=100\n",
    "\n",
    "x = Input(batch_shape=(None, imgSize, imgSize,1))\n",
    "conv = Convolution2D(48, 5, 5, border_mode='same', W_regularizer=l2(0.01))(x)\n",
    "conv = BatchNormalization()(conv)\n",
    "conv = Activation('relu')(conv)\n",
    "conv = MaxPooling2D(pool_size = (2,2), strides=(2,2))(conv)\n",
    "conv = Dropout(0.4)(conv)\n",
    "\n",
    "conv = Convolution2D(64, 5,5, border_mode='same', W_regularizer=l2(0.01))(conv)\n",
    "conv = BatchNormalization()(conv)\n",
    "conv = Activation('relu')(conv)\n",
    "conv = MaxPooling2D(pool_size = (2,2), strides=(2,2))(conv)\n",
    "conv = Dropout(0.4)(conv)\n",
    "\n",
    "conv = Convolution2D(128, 5,5, border_mode='same', W_regularizer=l2(0.01))(conv)\n",
    "conv = BatchNormalization()(conv)\n",
    "conv = Activation('relu')(conv)\n",
    "conv = MaxPooling2D(pool_size = (2,2), strides=(2,2))(conv)\n",
    "conv = Dropout(0.4)(conv)\n",
    "\n",
    "conv = Convolution2D(160,5,5, border_mode='same', W_regularizer=l2(0.01))(conv)\n",
    "conv = BatchNormalization()(conv)\n",
    "conv = Activation('relu')(conv)\n",
    "conv = MaxPooling2D(pool_size = (2,2), strides=(2,2))(conv)\n",
    "conv = Dropout(0.4)(conv)\n",
    "\n",
    "conv = Convolution2D(192,5,5, border_mode='same', W_regularizer=l2(0.01))(conv)\n",
    "conv = BatchNormalization()(conv)\n",
    "conv = Activation('relu')(conv)\n",
    "conv = MaxPooling2D(pool_size = (2,2), strides=(2,2))(conv)\n",
    "conv = Dropout(0.4)(conv)\n",
    "\n",
    "# conv = Convolution2D(192,5,5, border_mode='same', W_regularizer=l2(0.01))(conv)\n",
    "# conv = MaxPooling2D(pool_size = (2,2), strides=(1,1))(conv)\n",
    "# conv = Convolution2D(192,5,5, border_mode='same', W_regularizer=l2(0.01))(conv)\n",
    "# conv = MaxPooling2D(pool_size = (2,2), strides=(2,2))(conv)\n",
    "# conv = Convolution2D(192,5,5, border_mode='same', W_regularizer=l2(0.01))(conv)\n",
    "# conv = MaxPooling2D(pool_size = (2,2), strides=(1,1))(conv)\n",
    "\n",
    "flat = Flatten()(conv)\n",
    "dense = Dense(1024, W_regularizer=l2(0.01))(flat)\n",
    "dense = Activation('relu')(dense)\n",
    "dense = Dense(1024, W_regularizer=l2(0.01))(dense)\n",
    "dense = Activation('tanh')(dense)\n",
    "dense = Dense(1024, W_regularizer=l2(0.01))(dense)\n",
    "dense = Activation('relu')(dense)\n",
    "outL = Dense(5, W_regularizer=l2(0.01))(dense)\n",
    "outL = Activation('softmax', name=\"Length\")(outL)\n",
    "\n",
    "merged = merge([outL, dense], mode='concat')\n",
    "merged = Dense(1024, W_regularizer=l2(0.01))(merged)\n",
    "merged = Activation('relu')(merged)\n",
    "\n",
    "outD0 = Dense(11, W_regularizer=l2(0.01))(merged)\n",
    "outD0 = Activation('softmax', name=\"Digit0\")(outD0)\n",
    "outD1 = Dense(11, W_regularizer=l2(0.01))(merged)\n",
    "outD1 = Activation('softmax', name=\"Digit1\")(outD1)\n",
    "outD2 = Dense(11, W_regularizer=l2(0.01))(merged)\n",
    "outD2 = Activation('softmax', name=\"Digit2\")(outD2)\n",
    "outD3 = Dense(11, W_regularizer=l2(0.01))(merged)\n",
    "outD3 = Activation('softmax', name=\"Digit3\")(outD3)\n",
    "outD4 = Dense(11, W_regularizer=l2(0.01))(merged)\n",
    "outD4 = Activation('softmax', name=\"Digit4\")(outD4)\n",
    "\n",
    "model = Model(input=x, output=[outL, outD0, outD1, outD2, outD3, outD4])\n",
    "\n",
    "\n",
    "# from keras.utils.visualize_util import plot\n",
    "# plot(model, to_file='model.png')\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "trainReshaped = trainImageData.reshape((-1,imgSize,imgSize,1))\n",
    "trainDigit0  = trainImageDigits[:,0,:]\n",
    "trainDigit1  = trainImageDigits[:,1,:]\n",
    "trainDigit2  = trainImageDigits[:,2,:]\n",
    "trainDigit3  = trainImageDigits[:,3,:]\n",
    "trainDigit4  = trainImageDigits[:,4,:]\n",
    "\n",
    "validationReshaped = validationImageData.reshape((-1,imgSize,imgSize,1))\n",
    "validationDigit0  = validationImageDigits[:,0,:]\n",
    "validationDigit1  = validationImageDigits[:,1,:]\n",
    "validationDigit2  = validationImageDigits[:,2,:]\n",
    "validationDigit3  = validationImageDigits[:,3,:]\n",
    "validationDigit4  = validationImageDigits[:,4,:]\n",
    "model.fit(trainReshaped, [trainImageLengths, trainDigit0, trainDigit1, trainDigit2, trainDigit3, trainDigit4], nb_epoch=epochs, batch_size=batch_size, validation_data=(validationReshaped,[validationImageLengths,validationDigit0,validationDigit1,validationDigit2,validationDigit3,validationDigit4]))\n",
    "model.save('actualModel.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "_Describe how you set up the training and testing data for your model. How does the model perform on a realistic dataset?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "_What changes did you have to make, if any, to achieve \"good\" results? Were there any options you explored that made the results worse?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "_What were your initial and final results with testing on a realistic dataset? Do you believe your model is doing a good enough job at classifying numbers correctly?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Step 3: Test a Model on Newly-Captured Images\n",
    "\n",
    "Take several pictures of numbers that you find around you (at least five), and run them through your classifier on your computer to produce example results. Alternatively (optionally), you can try using OpenCV / SimpleCV / Pygame to capture live images from a webcam and run those through your classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "Use the code cell (or multiple code cells, if necessary) to implement the first step of your project. Once you have completed your implementation and are satisfied with the results, be sure to thoroughly answer the questions that follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "### Your code implementation goes here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "_Choose five candidate images of numbers you took from around you and provide them in the report. Are there any particular qualities of the image(s) that might make classification difficult?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "_Is your model able to perform equally well on captured pictures or a live camera stream when compared to testing on the realistic dataset?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Question 9\n",
    "_If necessary, provide documentation for how an interface was built for your model to load and classify newly-acquired images._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** Leave blank if you did not complete this part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Step 4: Explore an Improvement for a Model\n",
    "\n",
    "There are many things you can do once you have the basic classifier in place. One example would be to also localize where the numbers are on the image. The SVHN dataset provides bounding boxes that you can tune to train a localizer. Train a regression loss to the coordinates of the bounding box, and then test it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "Use the code cell (or multiple code cells, if necessary) to implement the first step of your project. Once you have completed your implementation and are satisfied with the results, be sure to thoroughly answer the questions that follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "### Your code implementation goes here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10\n",
    "_How well does your model localize numbers on the testing set from the realistic dataset? Do your classification results change at all with localization included?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 11\n",
    "_Test the localization function on the images you captured in **Step 3**. Does the model accurately calculate a bounding box for the numbers in the images you found? If you did not use a graphical interface, you may need to investigate the bounding boxes by hand._ Provide an example of the localization created on a captured image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Optional Step 5: Build an Application or Program for a Model\n",
    "Take your project one step further. If you're interested, look to build an Android application or even a more robust Python program that can interface with input images and display the classified numbers and even the bounding boxes. You can for example try to build an augmented reality app by overlaying your answer on the image like the [Word Lens](https://en.wikipedia.org/wiki/Word_Lens) app does.\n",
    "\n",
    "Loading a TensorFlow model into a camera app on Android is demonstrated in the [TensorFlow Android demo app](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android), which you can simply modify.\n",
    "\n",
    "If you decide to explore this optional route, be sure to document your interface and implementation, along with significant results you find. You can see the additional rubric items that you could be evaluated on by [following this link](https://review.udacity.com/#!/rubrics/413/view)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Implementation\n",
    "Use the code cell (or multiple code cells, if necessary) to implement the first step of your project. Once you have completed your implementation and are satisfied with the results, be sure to thoroughly answer the questions that follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "### Your optional code implementation goes here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentation\n",
    "Provide additional documentation sufficient for detailing the implementation of the Android application or Python program for visualizing the classification of numbers in images. It should be clear how the program or application works. Demonstrations should be provided. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Write your documentation here._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: Once you have completed all of the code implementations and successfully answered each question above, you may finalize your work by exporting the iPython Notebook as an HTML document. You can do this by using the menu above and navigating to  \n",
    "**File -> Download as -> HTML (.html)**. Include the finished document along with this notebook as your submission."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
